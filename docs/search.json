[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fusionACS",
    "section": "",
    "text": "Project overview\nA large amount of data concerning the experiences and wellbeing of American households is collected by surveys. Household surveys typically focus on a single topic – e.g. finances, housing, health – and field independent and relatively small samples. As a result, data users are often constrained by the particular variables and spatial resolution available in a single survey.\nThe fusionACS data science platform (Ummel et al. 2024) helps address this problem by statistically “fusing” microdata from disparate surveys to simulate a single, integrated, high-resolution survey. The resulting fused microdata can be used to perform analyses that would otherwise be impossible. At its core, fusionACS seeks to maximize the amount of useful information that can be extracted from the existing array of U.S. survey data.\nIn 2025, an enhanced version of fusionACS was introduced that integrates UrbanPop, a synthetic population data product produced by Oak Ridge National Laboratory (Tuccillo et al. 2023). UrbanPop provides probabilistic estimates of the location (block group) of each ACS respondent household. The fusionACS + UrbanPop platform is able to generate estimates for any donor survey variable for locales as small as individual census block groups.\n\n\nMethodology\nfusionACS uses the American Community Survey (ACS) – the largest U.S. household survey – as the “data backbone” of the fusion process. Variables in “donor” surveys are fused onto ACS Public Use Microdata Sample (PUMS) microdata to produce simulated values for variables unique to the donor. This generates probabilistic estimates of how ACS respondents might have answered a donor survey’s questionnaire. Respondent characteristics that are common to both the donor and the ACS (e.g. income, age, household size) – as well as spatial information that can be merged to both (e.g. characteristics of the local built environment) – are used as predictors variables in LightGBM machine learning models (Ke et al. 2017).\nSee the Methodology page for more information.\n\n\n\n\n\n\n\n\nUse cases\nSee the Publications page for more information.\n\n\n\n\n\nReferences\n\nKe, Guolin, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. 2017. “LightGBM: A Highly Efficient Gradient Boosting Decision Tree.” In Advances in Neural Information Processing Systems 30, 3149–57. https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.\n\n\nTuccillo, Joseph V., Robert Stewart, Amy Rose, Nathan Trombley, Jessica Moehl, Nicholas Nagle, and Budhendra Bhaduri. 2023. “UrbanPop: A Spatial Microsimulation Framework for Exploring Demographic Influences on Human Dynamics.” Applied Geography 151: 102844. https://doi.org/10.1016/j.apgeog.2022.102844.\n\n\nUmmel, Kevin, Miguel Poblete-Cazenave, Karthik Akkiraju, Nick Graetz, Hero Ashman, Cora Kingdon, Steven Herrera Tenorio, Aaryaman Sunny Singhal, Daniel Aldana Cohen, and Narasimha D. Rao. 2024. “Multidimensional Well-Being of US Households at a Fine Spatial Scale Using Fused Household Surveys.” Scientific Data 11 (142). https://doi.org/10.1038/s41597-023-02788-7."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "Frequently Asked Questions",
    "section": "",
    "text": "Can other donor surveys be fused?\nYes. In principle, any U.S. household- or person-level national survey circa 2005 or later is a candidate for fusion."
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Using the fusionACS package",
    "section": "",
    "text": "The experimental fusionACS R package allows users to access and analyze a “pseudo-sample” of the complete fusionACS database stored within the Yale High Performance Computing facility.\nThe complete database is prohibitively large for public dissemination and includes some data that cannot be shared. The pseudo-sample is an approximately 5% sample of ACS respondents (households and persons) for the period 2015-2019. In addition to all variables found in the ACS, it also includes all variables fused from donor surveys – currently limited to RECS 2020 for beta testing.\nImportantly, the sample includes plausible geographic identifiers – from census region down to individual census tracts – for each ACS household, obtained by sampling the underlying UrbanPop data in a way that ensures that all tracts nationwide are represented.\nSince the sample includes only a single fusion implicate, in addition to the other restrictions, the total amount of data released is only about 0.03% of the complete fusionACS database.\nWhile the sample data cannot be used to derive valid estimates, it can be used to understand the structure of the data, perform exploratory analysis, and design, refine, and test analyses. Our hope is that, eventually, it will be possible to remotely execute a valid analysis – designed and tested locally – using the complete fusionACS database and return the full “production” results to the user.\n\nPackage install and setup\nInstall the latest package version from Github.\ndevtools::install_github(\"ummel/fusionACS\")\nLoad the package.\n\nlibrary(fusionACS)\n\nDownload the latest fusionACS microdata psudeo-sample.\nget_microdata()\nThe data is automatically downloaded to a system-specific (and project-independent) location identified by the ‘rappdirs’ package. The path to the data files is accessible via get_directory(), but there is no particular reason to access it directly.\n\n\nAssemble microdata\nYou can view the data dictionary to see which surveys, year, and variables are available.\ndict = dictionary()\nView(dict)\nUse the assemble() function to obtain your desired subset of the pseudo-sample.\n\nExample 1\nAssemble household income (hincp), housing tenure (ten), and state of residence from the ACS, plus natural gas consumption (btung), square footage (totsqft_en), and the main space heating equipment type (equipm) from the 2020 RECS, plus pseudo-assignment of county and tract from UrbanPop. Return nationwide household data for ACS respondents in year 2019.\n\nmy.data = assemble(\n    variables = c(hincp, ten, btung, totsqft_en, equipm, state_name, county10, tract10), \n    year = 2019, \n    respondent = \"household\"\n)\n\nhead(my.data)\n\nKey: &lt;year, hid&gt;\n    year      hid weight  hincp\n   &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1:  2019 10000043    126  54548\n2:  2019 10000061    159  64346\n3:  2019 10000070    160 281830\n4:  2019 10000071     95  39396\n5:  2019 10000102    224  40002\n6:  2019 10000114     62  63639\n                                                       ten btung totsqft_en\n                                                    &lt;fctr&gt; &lt;int&gt;      &lt;int&gt;\n1:                                                  Rented     0        750\n2:                                                  Rented     5       1250\n3:                                    Owned free and clear 52200       1820\n4:                                    Owned free and clear     0        900\n5:                                                  Rented 31300        970\n6: Owned with mortgage or loan (include home equity loans)     0       1860\n              equipm state_name county10 tract10\n              &lt;fctr&gt;     &lt;char&gt;    &lt;int&gt;   &lt;int&gt;\n1:   Central furnace      Texas      179  950700\n2:   Central furnace    Arizona       17  942500\n3:   Central furnace   Oklahoma      143    7801\n4:   Central furnace    Indiana       77  966200\n5:   Central furnace   Kentucky       67     900\n6: Central heat pump    Florida       31   14500\n\n\n\n\nExample 2\nSame as above but for years 2017-2019 and includes optional expressions to: 1) Restrict to households in the state of Texas that used natural gas; 2) Create a new variable (btung_per_ft2) that measures consumption per square foot; and 3) Remove btung and totsqft_en after creating the new variable, for convenience.\n\nmy.data = assemble(\n  variables = c(hincp, ten, btung, totsqft_en, equipm, state_name, county10, tract10), \n  year = 2017:2019, \n  respondent = \"household\", \n  btung &gt; 0, \n  state_name == \"Texas\", \n  btung_per_ft2 = btung / totsqft_en, \n  -c(btung, totsqft_en)\n)\n\nhead(my.data)\n\nKey: &lt;year, hid&gt;\n    year      hid weight  hincp\n   &lt;int&gt;    &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1:  2017 10000422    343 168869\n2:  2017 10001536    172  37414\n3:  2017 10001571    350  35189\n4:  2017 10001689     75  27100\n5:  2017 10001697    135  43481\n6:  2017 10001735    158  60671\n                                                       ten\n                                                    &lt;fctr&gt;\n1: Owned with mortgage or loan (include home equity loans)\n2: Owned with mortgage or loan (include home equity loans)\n3:                                                  Rented\n4:                                    Owned free and clear\n5:                                                  Rented\n6:                                    Owned free and clear\n                                                                        equipm\n                                                                        &lt;fctr&gt;\n1: Built-in electric units installed in walls, ceilings, baseboards, or floors\n2:                                                             Central furnace\n3:                                                             Central furnace\n4:                                                             Central furnace\n5:                                                             Central furnace\n6:                                                        Wood or pellet stove\n   state_name county10 tract10 btung_per_ft2\n       &lt;char&gt;    &lt;int&gt;   &lt;int&gt;         &lt;num&gt;\n1:      Texas      245     302    16.8160920\n2:      Texas      367  140411    20.0000000\n3:      Texas      141    4002    37.6000000\n4:      Texas      201  340600    18.6192469\n5:      Texas      475  950200     0.9818182\n6:      Texas      113   15600    19.2380952\n\n\n\n\n\nAnalyze microdata\nUse the analyze() function to calculate means, medians, sums, proportions, and counts of specific variables, optionally across population subgroups. The analysis process uses the microdata sample you generated via assemble().\n\nExample 1\nCalculate mean natural gas consumption per square foot. Since no by argument is specified, the analysis applies to all observations in my.data; i.e. all households in Texas in 2017-2019 that used natural gas.\n\ntest &lt;- analyze(\n  data = my.data,\n  ~ mean(btung_per_ft2)\n)\n\nComputing estimates for numerical analyses:\n ~ mean(btung_per_ft2) \n\ntest\n\n# A tibble: 1 × 12\n  lhs         rhs   type  level N_eff   ubar b       est moe   se    df    cv   \n  &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 mean_btung… mean… mean  NA     5190 0.0444 NA     20.3 NA    NA    NA    NA   \n\n\nThe result has a single row, because no sub-populations were requested in this example. The results include a point estimate (est), but this is only an approximation since it is computed using a fraction of the complete database. No margin of error (moe) is returned, because the pseudo-sample does not contain the multiple fusion implicates necessary to estimate uncertainty.\n\n\nExample 2\nSame as above but also request median natural gas consumption per square foot and the proportion of households using each type of heating equipment (equipm). Calculate estimates for sub-populations defined by housing tenure (ten).\n\ntest &lt;- analyze(\n  data = my.data,\n  ~ mean(btung_per_ft2),\n  ~ median(btung_per_ft2),\n  ~ mean(equipm),\n  by = ten\n)\n\nComputing estimates for categorical analyses:\n ~ mean(equipm) \nComputing estimates for numerical analyses:\n ~ mean(btung_per_ft2)\n ~ median(btung_per_ft2) \n\ntest\n\n# A tibble: 48 × 13\n   lhs    rhs   type  ten   level  N_eff    ubar b         est moe   se    df   \n   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;   &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n 1 mean_… mean… mean  Occu… &lt;NA&gt;    94.4 2.70e+0 NA    2.22e+1 NA    NA    NA   \n 2 mean_… mean… mean  Owne… &lt;NA&gt;  1848.  1.30e-1 NA    2.01e+1 NA    NA    NA   \n 3 mean_… mean… mean  Owne… &lt;NA&gt;  2178.  7.61e-2 NA    1.87e+1 NA    NA    NA   \n 4 mean_… mean… mean  Rent… &lt;NA&gt;  1129.  2.80e-1 NA    2.33e+1 NA    NA    NA   \n 5 media… medi… medi… Occu… &lt;NA&gt;    94.4 2.56e+0 NA    1.78e+1 NA    NA    NA   \n 6 media… medi… medi… Owne… &lt;NA&gt;  1848.  9.69e-2 NA    1.69e+1 NA    NA    NA   \n 7 media… medi… medi… Owne… &lt;NA&gt;  2178.  6.26e-2 NA    1.63e+1 NA    NA    NA   \n 8 media… medi… medi… Rent… &lt;NA&gt;  1129.  1.81e-1 NA    1.95e+1 NA    NA    NA   \n 9 mean_… mean… prop  Occu… No s…   94.4 2.35e-4 NA    9.00e-3 NA    NA    NA   \n10 mean_… mean… prop  Occu… Cent…   94.4 2.03e-3 NA    7.48e-1 NA    NA    NA   \n# ℹ 38 more rows\n# ℹ 1 more variable: cv &lt;lgl&gt;\n\n\nThe results suggest the typical (median) renter in Texas consumes more natural gas per square foot of living space than homeowners.\n\nsubset(test, rhs == \"median(btung_per_ft2)\", select = c(ten, est))\n\n# A tibble: 4 × 2\n  ten                                                       est\n  &lt;chr&gt;                                                   &lt;dbl&gt;\n1 Occupied without payment of rent                         17.8\n2 Owned free and clear                                     16.9\n3 Owned with mortgage or loan (include home equity loans)  16.3\n4 Rented                                                   19.5\n\n\n\n\nExample 3\nMean and median natural gas consumption per square foot, calculated (separately) for population subgroups defined by: 1) housing tenure; 2) housing tenure and heating equipment; and 3) census tract. This example illustrates how flexible the by argument can be.\n\ntest &lt;- analyze(\n  data = my.data,\n  ~ mean(btung_per_ft2),\n  ~ median(btung_per_ft2),\n  by = list(ten, c(ten, equipm), c(state_name, county10, tract10))\n)\n\nComputing estimates for numerical analyses:\n ~ mean(btung_per_ft2)\n ~ median(btung_per_ft2)"
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "Methodological Details",
    "section": "",
    "text": "A fusionACS analysis consists of a request for an estimate (mean, median, sum, proportion, or count) of a particular variable across one or more sub-populations. For example, “Mean household electricity consumption in Chicago, by census tract”; or “Proportion of low-income households in Atlanta without air conditioning”.\nIn addition to a point estimate, fusionACS also returns the associated uncertainty or margin of error. There are two fundamental sources of uncertainty when performing an analysis using fusionACS data. First, there is uncertainty in the fused outcomes for ACS respondent households; i.e. uncertainty in the values predicted by the underlying machine learning models. Second, there is uncertainty in the assigned weight for each respondent; i.e. uncertainty about how often a sampled respondent “type” appears in the population.\nOutcome uncertainty is captured through the production of M unique implicates during the fusion process; each implicate is a plausible simulation of the fused variables given uncertainty in the underlying models. Weighting uncertainty is captured through the use of R unique replicates; each replicate provides plausible household sample weights given uncertainty in the characteristics of the population.\nTaken together, a fusionACS analysis computes across \\(k = M \\cdot R\\) unique samples, each representing a plausible combination of outcomes and weights. By assessing the variance of these samples, it is possible to estimate the margin of error associated with any given point estimate.\n\n\nFor any given fusionACS analysis, the point estimates and associated margin of error are calculated using the technique of Rubin (1987). The point estimate, \\(\\bar{\\theta}\\), is the mean of the individual estimates calculated for each of the \\(k\\) samples:\n\\[\n\\bar{\\theta} = \\frac{1}{k} \\sum_{i=1}^{k} \\hat{\\theta}_i\n\\] The variance of \\(\\bar{\\theta}\\) is calculated by “pooling” the variance both within and between samples:\n\\[\n\\text{Var}_{\\text{combined}} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{Var}( \\hat{\\theta}_i ) + \\left( 1 + \\frac{1}{k} \\right) \\text{Var}_{\\text{between}}\n\\]\nwhere \\(\\text{Var}_{\\text{between}}\\) is the “between-sample” variance of the individual estimates:\n\\[\n\\text{Var}_{\\text{between}} = \\frac{1}{k - 1} \\sum_{i=1}^{k} \\left( \\hat{\\theta}_i - \\bar{\\theta} \\right)^2\n\\]\nand \\(\\text{Var}( \\hat{\\theta}_i )\\) refers to the “within-sample” variances of the \\(k\\) individual estimates; i.e. the square of the standard error [\\(\\text{SE}( \\hat{\\theta} )^2\\)].\n\n\n\nThe calculation of within-sample standard errors depends on the type of estimate requested. To calculate the standard error of a mean estimate for a sample of \\(n\\) observations with frequency weights \\(w_i\\):\n\\[\nSE(\\hat{\\theta}_{\\text{mean}}) = \\sqrt{ \\frac{ \\sum w_i (x_i - \\bar{x}_w)^2 }{ \\sum w_i - 1 } \\cdot \\frac{1}{n_{\\text{eff}}} }\n\\]where \\(\\bar{x}_w\\) is the weighted mean and \\(n_{\\text{eff}}\\) is the effective sample size calculated from the observation weights:\n\\[\nn_{\\text{eff}} = \\frac{ \\left( \\sum w_i \\right)^2 }{ \\sum w_i^2 }\n\\]\nThe use of \\(n_{\\text{eff}}\\) (rather than sample size \\(n\\)) accounts for additional uncertainty introduced by variance in the weights themselves. In general, the smaller the target population the greater the “unevenness” of the weights, leading to \\(n_{\\text{eff}}&lt;n\\), and a corresponding increase in the standard error (Kish 1965; Lumley 2010).\nThe implementation in R uses the collapse package qsu() function (Krantz 2025), which computes the mean, standard deviation, and sum of weights in a single pass through the data in C/C++. This provides extremely fast calculation, even when the data is grouped across \\(k\\) samples and (optionally) user-specified sub-populations.\nFor median estimates, calculation of the standard error via bootstrapping requires significant computation (Hahn and Meeker 1991). Given the need for computational efficiency in the fusionACS context, the standard error of the median is instead estimated using the large-sample approximation:\n\\[\nSE(\\hat{\\theta}_{\\text{median}}) \\approx \\frac{1}{2f(m)} \\cdot \\frac{1}{\\sqrt{n_{\\text{eff}}}}\n\\] where \\(f(m)\\) is the density at the median. The formula arises from the asymptotic variance of sample quantiles under regularity conditions (Serfling 1980). For speed, \\(f(m)\\) is approximated using a finite difference of the quantile function (Koenker 2005):\n\\[\nf(m) \\approx \\frac{p_2 - p_1}{Q(p_2) - Q(p_1)}\n\\]\nwhere \\(Q(p)\\) is the weighted quantile function, and \\(p_1\\) and \\(p_2\\) are probabilities close to the median (0.475 and 0.525 by default). Implementation via the collapse package leverages a single, radix-based ordering of the input to efficiently compute the median, \\(Q(p_1)\\), and \\(Q(p_2)\\). This allows the standard error to be estimated at little additional cost beyond computation of the median itself. If \\(f(m)\\) is undefined, the code falls back to the conservative approximation of Tukey (1977):\n\\[\nSE(\\hat{\\theta}_{\\text{median}}) \\approx SE(\\hat{\\theta}_{\\text{mean}})  \\cdot \\frac{\\pi}{2}\n\\] To calculate the standard error of a proportion, given the previously-defined effective sample size:\n\\[\nSE(\\hat{\\theta}_{\\text{proportion}}) = \\sqrt{ \\frac{ \\hat{p}^*(1 - \\hat{p}^*) }{ n_{\\text{eff}} } }\n\\]\nwhere \\(\\hat{p}^*\\) is the Agresti-Coull (Agresti and Coull 1998) adjusted proportion derived from the weighted sample proportion (\\(\\hat{p}\\)), assuming a 90% confidence interval (\\(z=1.645\\)):\n\\[\n\\hat{p}^* = \\frac{ \\hat{p} \\cdot n_{\\text{eff}} + \\frac{z^2}{2} }{ n_{\\text{eff}} + z^2 }\n\\]\nThis adjustment ensures a non-zero standard error when \\(\\hat{p}(1 - \\hat{p})\\) is zero; e.g. for unobserved outcomes in smaller samples. The un-adjusted sample proportion (\\(\\hat{p}\\)) is always returned as the point estimate.\nFor sums (numerical case) and counts (categorical case), the standard error is a multiple of the standard error of the mean and proportion, respectively:\n\\[\nSE(\\hat{\\theta}_{\\text{sum}}) = SE(\\hat{\\theta}_{\\text{mean}}) \\cdot \\sum w_i\n\\]\n\\[\nSE(\\hat{\\theta}_{\\text{count}}) = SE(\\hat{\\theta}_{\\text{proportion}}) \\cdot \\sum w_i\n\\]\n\n\n\nHaving calculated \\(\\text{Var}_{\\text{combined}}\\) and its component variances, the degrees of freedom is calculated using the formula of Barnard and Rubin (1999). Compared to the original Rubin (1987) degrees of freedom, this formulation allows for unequal within-sample variances and is more accurate for small samples.\n\\[\n\\nu = (k - 1) \\left( 1 + \\frac{ \\frac{1}{k} \\sum_{i=1}^{k} \\text{Var}(\\hat{\\theta}_i) }{ \\text{Var}_{\\text{between}} } \\right)^2\n\\bigg/\n\\left( \\frac{1}{k - 1} + \\frac{1}{k^2} \\cdot \\frac{ \\sum_{i=1}^{k} \\text{Var}(\\hat{\\theta}_i)^2 }{ \\text{Var}_{\\text{between}}^2 } \\right)\n\\]\nIn keeping with the convention used by the U.S. Census Bureau for published ACS estimates, fusionACS returns the 90% margin of error and the coefficient of variation.\n\\[\n\\text{ME}_{90\\%} = t_{0.95, \\, \\nu} \\cdot \\sqrt{\\text{Var}_{\\text{combined}}}\n\\] \\[\n\\text{CV} = 100 \\times \\frac{\\text{ME}_{90\\%} / 1.645}{|\\bar{\\theta}|}\n\\] The latter provides a scale-independent measure of estimate reliability."
  },
  {
    "objectID": "methodology.html#estimating-uncertainty",
    "href": "methodology.html#estimating-uncertainty",
    "title": "Methodological Details",
    "section": "",
    "text": "A fusionACS analysis consists of a request for an estimate (mean, median, sum, proportion, or count) of a particular variable across one or more sub-populations. For example, “Mean household electricity consumption in Chicago, by census tract”; or “Proportion of low-income households in Atlanta without air conditioning”.\nIn addition to a point estimate, fusionACS also returns the associated uncertainty or margin of error. There are two fundamental sources of uncertainty when performing an analysis using fusionACS data. First, there is uncertainty in the fused outcomes for ACS respondent households; i.e. uncertainty in the values predicted by the underlying machine learning models. Second, there is uncertainty in the assigned weight for each respondent; i.e. uncertainty about how often a sampled respondent “type” appears in the population.\nOutcome uncertainty is captured through the production of M unique implicates during the fusion process; each implicate is a plausible simulation of the fused variables given uncertainty in the underlying models. Weighting uncertainty is captured through the use of R unique replicates; each replicate provides plausible household sample weights given uncertainty in the characteristics of the population.\nTaken together, a fusionACS analysis computes across \\(k = M \\cdot R\\) unique samples, each representing a plausible combination of outcomes and weights. By assessing the variance of these samples, it is possible to estimate the margin of error associated with any given point estimate.\n\n\nFor any given fusionACS analysis, the point estimates and associated margin of error are calculated using the technique of Rubin (1987). The point estimate, \\(\\bar{\\theta}\\), is the mean of the individual estimates calculated for each of the \\(k\\) samples:\n\\[\n\\bar{\\theta} = \\frac{1}{k} \\sum_{i=1}^{k} \\hat{\\theta}_i\n\\] The variance of \\(\\bar{\\theta}\\) is calculated by “pooling” the variance both within and between samples:\n\\[\n\\text{Var}_{\\text{combined}} = \\frac{1}{k} \\sum_{i=1}^{k} \\text{Var}( \\hat{\\theta}_i ) + \\left( 1 + \\frac{1}{k} \\right) \\text{Var}_{\\text{between}}\n\\]\nwhere \\(\\text{Var}_{\\text{between}}\\) is the “between-sample” variance of the individual estimates:\n\\[\n\\text{Var}_{\\text{between}} = \\frac{1}{k - 1} \\sum_{i=1}^{k} \\left( \\hat{\\theta}_i - \\bar{\\theta} \\right)^2\n\\]\nand \\(\\text{Var}( \\hat{\\theta}_i )\\) refers to the “within-sample” variances of the \\(k\\) individual estimates; i.e. the square of the standard error [\\(\\text{SE}( \\hat{\\theta} )^2\\)].\n\n\n\nThe calculation of within-sample standard errors depends on the type of estimate requested. To calculate the standard error of a mean estimate for a sample of \\(n\\) observations with frequency weights \\(w_i\\):\n\\[\nSE(\\hat{\\theta}_{\\text{mean}}) = \\sqrt{ \\frac{ \\sum w_i (x_i - \\bar{x}_w)^2 }{ \\sum w_i - 1 } \\cdot \\frac{1}{n_{\\text{eff}}} }\n\\]where \\(\\bar{x}_w\\) is the weighted mean and \\(n_{\\text{eff}}\\) is the effective sample size calculated from the observation weights:\n\\[\nn_{\\text{eff}} = \\frac{ \\left( \\sum w_i \\right)^2 }{ \\sum w_i^2 }\n\\]\nThe use of \\(n_{\\text{eff}}\\) (rather than sample size \\(n\\)) accounts for additional uncertainty introduced by variance in the weights themselves. In general, the smaller the target population the greater the “unevenness” of the weights, leading to \\(n_{\\text{eff}}&lt;n\\), and a corresponding increase in the standard error (Kish 1965; Lumley 2010).\nThe implementation in R uses the collapse package qsu() function (Krantz 2025), which computes the mean, standard deviation, and sum of weights in a single pass through the data in C/C++. This provides extremely fast calculation, even when the data is grouped across \\(k\\) samples and (optionally) user-specified sub-populations.\nFor median estimates, calculation of the standard error via bootstrapping requires significant computation (Hahn and Meeker 1991). Given the need for computational efficiency in the fusionACS context, the standard error of the median is instead estimated using the large-sample approximation:\n\\[\nSE(\\hat{\\theta}_{\\text{median}}) \\approx \\frac{1}{2f(m)} \\cdot \\frac{1}{\\sqrt{n_{\\text{eff}}}}\n\\] where \\(f(m)\\) is the density at the median. The formula arises from the asymptotic variance of sample quantiles under regularity conditions (Serfling 1980). For speed, \\(f(m)\\) is approximated using a finite difference of the quantile function (Koenker 2005):\n\\[\nf(m) \\approx \\frac{p_2 - p_1}{Q(p_2) - Q(p_1)}\n\\]\nwhere \\(Q(p)\\) is the weighted quantile function, and \\(p_1\\) and \\(p_2\\) are probabilities close to the median (0.475 and 0.525 by default). Implementation via the collapse package leverages a single, radix-based ordering of the input to efficiently compute the median, \\(Q(p_1)\\), and \\(Q(p_2)\\). This allows the standard error to be estimated at little additional cost beyond computation of the median itself. If \\(f(m)\\) is undefined, the code falls back to the conservative approximation of Tukey (1977):\n\\[\nSE(\\hat{\\theta}_{\\text{median}}) \\approx SE(\\hat{\\theta}_{\\text{mean}})  \\cdot \\frac{\\pi}{2}\n\\] To calculate the standard error of a proportion, given the previously-defined effective sample size:\n\\[\nSE(\\hat{\\theta}_{\\text{proportion}}) = \\sqrt{ \\frac{ \\hat{p}^*(1 - \\hat{p}^*) }{ n_{\\text{eff}} } }\n\\]\nwhere \\(\\hat{p}^*\\) is the Agresti-Coull (Agresti and Coull 1998) adjusted proportion derived from the weighted sample proportion (\\(\\hat{p}\\)), assuming a 90% confidence interval (\\(z=1.645\\)):\n\\[\n\\hat{p}^* = \\frac{ \\hat{p} \\cdot n_{\\text{eff}} + \\frac{z^2}{2} }{ n_{\\text{eff}} + z^2 }\n\\]\nThis adjustment ensures a non-zero standard error when \\(\\hat{p}(1 - \\hat{p})\\) is zero; e.g. for unobserved outcomes in smaller samples. The un-adjusted sample proportion (\\(\\hat{p}\\)) is always returned as the point estimate.\nFor sums (numerical case) and counts (categorical case), the standard error is a multiple of the standard error of the mean and proportion, respectively:\n\\[\nSE(\\hat{\\theta}_{\\text{sum}}) = SE(\\hat{\\theta}_{\\text{mean}}) \\cdot \\sum w_i\n\\]\n\\[\nSE(\\hat{\\theta}_{\\text{count}}) = SE(\\hat{\\theta}_{\\text{proportion}}) \\cdot \\sum w_i\n\\]\n\n\n\nHaving calculated \\(\\text{Var}_{\\text{combined}}\\) and its component variances, the degrees of freedom is calculated using the formula of Barnard and Rubin (1999). Compared to the original Rubin (1987) degrees of freedom, this formulation allows for unequal within-sample variances and is more accurate for small samples.\n\\[\n\\nu = (k - 1) \\left( 1 + \\frac{ \\frac{1}{k} \\sum_{i=1}^{k} \\text{Var}(\\hat{\\theta}_i) }{ \\text{Var}_{\\text{between}} } \\right)^2\n\\bigg/\n\\left( \\frac{1}{k - 1} + \\frac{1}{k^2} \\cdot \\frac{ \\sum_{i=1}^{k} \\text{Var}(\\hat{\\theta}_i)^2 }{ \\text{Var}_{\\text{between}}^2 } \\right)\n\\]\nIn keeping with the convention used by the U.S. Census Bureau for published ACS estimates, fusionACS returns the 90% margin of error and the coefficient of variation.\n\\[\n\\text{ME}_{90\\%} = t_{0.95, \\, \\nu} \\cdot \\sqrt{\\text{Var}_{\\text{combined}}}\n\\] \\[\n\\text{CV} = 100 \\times \\frac{\\text{ME}_{90\\%} / 1.645}{|\\bar{\\theta}|}\n\\] The latter provides a scale-independent measure of estimate reliability."
  },
  {
    "objectID": "methodology.html#interpretation-of-uncertainty",
    "href": "methodology.html#interpretation-of-uncertainty",
    "title": "Methodological Details",
    "section": "Interpretation of uncertainty",
    "text": "Interpretation of uncertainty\nThere is no universal standard for judging when the margin of error and/or coefficient of variation are “too high”. The answer always depends on the nature of the analysis and intended application. At a minimum, generalized guidelines should be used to determine when and if valid conclusions can be drawn from specific estimates; for example, see Parmenter and Lau (2013), page 2.\nIt is important that analyses or reports based on fusionACS data products communicate the authors’ chosen standards for determining when an estimate is sufficiently reliable to draw conclusions."
  },
  {
    "objectID": "inputs.html",
    "href": "inputs.html",
    "title": "Data Inputs",
    "section": "",
    "text": "Donor surveys\nDescription of the data inputs."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Citing fusionACS\nAny use of fusionACS outputs should cite Ummel et al. (2024):\n\nUmmel, Kevin, Miguel Poblete-Cazenave, Karthik Akkiraju, Nick Graetz, Hero Ashman, Cora Kingdon, Steven Herrera Tenorio, Aaryaman Sunny Singhal, Daniel Aldana Cohen, and Narasimha D. Rao. 2024. “Multidimensional Well-Being of US Households at a Fine Spatial Scale Using Fused Household Surveys.” Scientific Data 11 (142). https://doi.org/10.1038/s41597-023-02788-7.\n\nAny use of fusionACS estimates for locales smaller than Public Use Microdata Areas (PUMA’s) should also cite Tuccillo et al. (2023):\n\nTuccillo, Joseph V., Robert Stewart, Amy Rose, Nathan Trombley, Jessica Moehl, Nicholas Nagle, and Budhendra Bhaduri. 2023. “UrbanPop: A Spatial Microsimulation Framework for Exploring Demographic Influences on Human Dynamics.” Applied Geography 151: 102844. https://doi.org/10.1016/j.apgeog.2022.102844.\n\n\n\nPublications using fusionACS\n\nBrown, M., Becker, J. M., Carbone, J. C., Goforth, T., McFarland, J., Nock, D., Pitman, K., & Steinberg, D. 2024. Tax credits for clean electricity: The distributional impacts of supply-push policies in the power sector. Journal of the Association of Environmental and Resource Economists, 11(S1), S199–S229. https://doi.org/10.1086/730923"
  },
  {
    "objectID": "team.html",
    "href": "team.html",
    "title": "Team",
    "section": "",
    "text": "Team members\nAbout the team members"
  },
  {
    "objectID": "outputs.html",
    "href": "outputs.html",
    "title": "Data Outputs",
    "section": "",
    "text": "Outputs\nData outputs available for visualization or download."
  }
]